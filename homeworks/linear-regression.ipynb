{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2: Linear Regression\n",
    "\n",
    "In this homework you will learn the concepts of linear regression by implementing it.\n",
    "\n",
    "Implement the body of each function and test whether you have done right for each of them or not by running the tests. Each function has a test code block just below its definition.\n",
    "\n",
    "- Remember: **m** = number of data items (size of the data set) and **n** = number of features\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import what we need\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Function\n",
    "\n",
    "For a single data item $ x_{1 \\times n} $, the hypothesis is the linear product of $ w_{n \\times 1} $ and $ x_{1 \\times n} $ in addition to bias $ b_{1 \\times 1} $. The result is a number:\n",
    "\n",
    "$$ h_{w,b}(x) = w^Tx + b $$\n",
    "\n",
    "For a data set $ X_{m \\times n} $ which contains multiple data items stacking vertically, the result is a vector $ h_{m \\times 1} $ which contains predections for all data items:\n",
    "\n",
    "$$ h_{w,b}(X) = Xw + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def h(X, w, b):\n",
    "    \"\"\"\n",
    "    The hypothesis function\n",
    "    \n",
    "    X: the data set matrix (m x n)\n",
    "    w: the weights vector (n x 1)\n",
    "    b: the bias (1 x 1)\n",
    "    \n",
    "    Return: a vector stacking predictions for all data items (m x 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE (~ 1 line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the hypothesis\n",
    "\n",
    "X, y, w, b = (np.array([[1, 2, 3], [4, 5, 6]]), np.array([[2], [3]]), np.array([[3], [4], [5]]), 5)\n",
    "\n",
    "hyp = h(X, w, b)\n",
    "true = np.array([[31], [67]])\n",
    "\n",
    "assert hyp.shape == (X.shape[0], 1), \\\n",
    "       'The result should be in shape ({}, 1). Currently is {}'.format(X.shape[0], hyp.shape)\n",
    "\n",
    "if np.allclose(hyp, true):\n",
    "    print('Hypothesis ok.')\n",
    "else:\n",
    "    print('Hypothesis does not work properly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Cost function for a linear regression model is [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) over data set:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "   \\begin{split}\n",
    "   J_{w,b}(X) &= \\frac{1}{2m}\\sum_{i=1}^m (h_{w,b}(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "   &= \\frac{1}{2m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "   \\end{split}\n",
    "   \\end{equation} $$\n",
    "\n",
    "The goal of linear regression is to minimize this cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean squared error cost function\n",
    "    \n",
    "    y_true: the vector of true labels of data items (m x 1)\n",
    "    y_pred: the vector of predictions of data items (m x 1)\n",
    "    \n",
    "    Return: a single number representing the cost\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE (~ 2 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test cost function\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X, y, w, b = (np.array([[1, 2, 3], [4, 5, 6]]), np.array([[2], [3]]), np.array([[3], [4], [5]]), 5)\n",
    "\n",
    "mse = mean_squared_error(y, h(X, w, b))\n",
    "cst = cost(y, h(X, w, b))\n",
    "\n",
    "if np.isclose(mse / 2, cst):\n",
    "    print('Cost function ok.')\n",
    "else:\n",
    "    print('Cost function does not work properly.')\n",
    "    print('Should\\'ve returned:', mse / 2)\n",
    "    print('Returned:', cst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent algorithm tries to find the minimum of a function by starting somewhere on the function and taking small steps through the gradient of the function.\n",
    "\n",
    "In linear regression, the function we are trying to minimize is the cost function $ J_{w,b}(X) $. The derivations are:\n",
    "\n",
    "$$ \\begin{equation}\n",
    "   \\begin{split}\n",
    "       \\frac{\\partial J_{w,b}(X)}{\\partial w_j} &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2m}\\sum_{i=1}^m (h_{w,b}(x^{(i)}) - y^{(i)})^2 \\\\\n",
    "       &= \\frac{1}{2m}\\sum_{i=1}^m 2 (h_{w,b}(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial w_j} h_{w,b}(x^{(i)}) \\\\\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (h_{w,b}(x^{(i)}) - y^{(i)}) x_{j}^{(i)} \\\\\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x_{j}^{(i)}\n",
    "   \\end{split}\n",
    "   \\end{equation} $$\n",
    "\n",
    "\n",
    "$$ \\begin{equation}\n",
    "   \\begin{split}\n",
    "       \\frac{\\partial J_{w,b}(X)}{\\partial b} &= \\frac{1}{m}\\sum_{i=1}^m \n",
    "       (h_{w,b}(x^{(i)}) - y^{(i)}) \\frac{\\partial}{\\partial b} h_{w,b}(x^{(i)}) \\\\\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (h_{w,b}(x^{(i)}) - y^{(i)}) \\\\\n",
    "       &= \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n",
    "   \\end{split}\n",
    "   \\end{equation} $$\n",
    "   \n",
    "- Actually these two derivations are the same except that in the second one, $ x_{0}^{(i)} = 1 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(X, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    The gradient of cost function\n",
    "    \n",
    "    X: the data set matrix (m x n)\n",
    "    y_true: the vector of true labels of data items (m x 1)\n",
    "    y_pred: the vector of predictions of data items (m x 1)\n",
    "    \n",
    "    Return: vector dJ/dw (n x 1) and number dJ/db (1 x 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE (~ 4 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, w, b = (np.array([[1, 2, 3], [4, 5, 6]]), np.array([[2], [3]]), np.array([[3], [4], [5]]), 5)\n",
    "\n",
    "true = (np.array([[142.5], [189], [235.5]]), 46.5)\n",
    "res = gradient(X, y, h(X, w, b))\n",
    "\n",
    "if np.allclose(res[0], true[0]) and np.isclose(res[1], true[1]):\n",
    "    print('Gradient function ok.')\n",
    "else:\n",
    "    print('Gradient function is not working properly.')\n",
    "    print('should output:', true)\n",
    "    print('Outputted:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(X, y_true, y_pred, w, b, alpha):\n",
    "    \"\"\"\n",
    "    This function updates parameters w and b according to their derivations.\n",
    "    It should compute the cost function derivations with respect to w and b first,\n",
    "        then take a step for each parameters in w and b.\n",
    "    \n",
    "    X: the data set matrix (m x n)\n",
    "    y_true: the vector of true labels of data items (m x 1)\n",
    "    y_pred: the vector of predictions of data items (m x 1)\n",
    "    w: the weights vector (n x 1)\n",
    "    b: the bias (1 x 1)\n",
    "    alpha: the learning rate\n",
    "    \n",
    "    Returns: the updated parameters w and b\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE (~ 4 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test update_parameters function\n",
    "\n",
    "X, y, w, b = (np.array([[1, 2, 3], [4, 5, 6]]), np.array([[2], [3]]), np.array([[3], [4], [5]]), 5)\n",
    "\n",
    "res = update_parameters(X, y, h(X, w, b), w, b, 0.01)\n",
    "true = (np.array([[1.575], [2.11], [2.645]]), 4.535)\n",
    "\n",
    "if np.allclose(res[0], true[0]) and np.isclose(res[1], true[1]):\n",
    "    print('Update parameters function ok.')\n",
    "else:\n",
    "    print('Update parameters function is not working properly.')\n",
    "    print('should output:', true)\n",
    "    print('Outputted:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha, n_iterations):\n",
    "    \"\"\"\n",
    "    The gradient descent algorithm:\n",
    "        1. initialize parameters w and b with zeros (not randoms)\n",
    "        for i in n_iterations:\n",
    "            2. compute the hypothesis h(X, w, b)\n",
    "            3. update the parameters using function update_parameters(X, y_true, y_pred, w, b, alpha)\n",
    "            4. compute the cost and see the cost is decreasing in each step (optional)\n",
    "            \n",
    "    X: the data set matrix (m x n)\n",
    "    y: the vector of true labels of data items (m x 1)\n",
    "    alpha: the learning rate\n",
    "    n_iterations: number of steps gradient descent should take to converge\n",
    "    \n",
    "    Returns: the best parameters w and b gradient descent found at last\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE GOES HERE (~ 7 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test gradient_descent function\n",
    "\n",
    "true = (np.array([[0.11532006], [0.19906458], [0.2828091]]), 0.083744520421231317)\n",
    "res = gradient_descent(X, y, 0.01, 20)\n",
    "\n",
    "if np.allclose(res[0], true[0]) and np.isclose(res[1], true[1]):\n",
    "    print('Gradient descent function ok.')\n",
    "else:\n",
    "    print('Gradient descent function is not working properly.')\n",
    "    print('should output:', true)\n",
    "    print('Outputted:', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import scale\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X = X[:, 3].reshape(X.shape[0], 1)\n",
    "X = scale(X)\n",
    "y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a linear regression model from sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train our linear regression model\n",
    "\n",
    "w, b = gradient_descent(X, y, 0.1, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the result\n",
    "\n",
    "plt.scatter(X, y, s=10, color='gray')\n",
    "plt.plot(X, h(X, w, b), color='red', label='Ours');\n",
    "plt.plot(X, model.predict(X), color='blue', label='Best');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis should be close to the best hypothesis. If they are not the same, raise `n_iterations` argument for our `gradient_descent` function and be careful of `alpha` the learning rate.\n",
    "\n",
    "How well your linear regression model is? How much happy are you!? :D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
